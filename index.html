<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rock-Paper-Scissors</title>

    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css"
        integrity="sha512fkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <body>
        <div class="wrapper-game">
            <h1>Challenge the Basilisk in paper-scissor-rock</h1>
            <div class="description">
                "Roko’s basilisk is a thought experiment proposed in 2010 by the user Roko on the Less Wrong community
                blog. Roko used ideas in decision theory to argue that a sufficiently powerful AI agent would have an
                incentive to torture anyone who imagined the agent but didn't work to bring the agent into existence.
                The argument was called a "basilisk" because merely hearing the argument would supposedly put you at
                risk of torture from this hypothetical agent — a basilisk in this context is any information that harms
                or endangers the people who hear it.
                Roko's argument was broadly rejected on Less Wrong, with commenters objecting that an agent like the one
                Roko was describing would have no real reason to follow through on its threat: once the agent already
                exists, it can't affect the probability of its existence, so torturing people for their past decisions
                would be a waste of resources. Although several decision theories allow one to follow through on acausal
                threats and promises — via the same precommitment methods that permit mutual cooperation in prisoner's
                dilemmas — it is not clear that such theories can be blackmailed. If they can be blackmailed, this
                additionally requires a large amount of shared information and trust between the agents, which does not
                appear to exist in the case of Roko's basilisk." "<a
                    href="https://www.lesswrong.com/tag/rokos-basilisk"> Read more on LessWrong </a>
            </div>

            <p> Roko's basilisk choice: <span class="material-icons " id="roko-choice"></span<</p> <p> Player's choice:
                    <span class="material-icons" id="player-choice"></span></p>
            <p id="result"></p>
            <div id="wrapper-choice">
                <button id="rock"><i class="fas fa-hand-rock"></i></button>
                <button id="paper"><span class="material-icons"> <span class="material-icons">
                            description</span></span></button>
                <button id="scissors"><span class="material-icons"> content_cut</span></button>
            </div>
        </div>

    </body>
    <script src="main.js"></script>
</body>

</html>